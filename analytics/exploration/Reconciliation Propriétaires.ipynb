{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install splink"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "from tqdm.auto import tqdm \n",
    "import pandas as pd\n",
    "\n",
    "connection_source = duckdb.connect(database=\"../dbt/database_name.duckdb\", read_only = True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# OK : Filtrer les propriétaires ayant un logements vacant dans la prod (lovac 2024)\n",
    "# Logement est le lien entre un propriétaire national et départemental \n",
    "# Aller chercher les logements via les ID locaux dans les fichiers fonciers\n",
    "# Aller chercher les logements via les ID locaux dans la prod "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "8e828231792040cfa68e9d0753cbbd97",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "2b741a0e930c40cdabd867c38579f84e",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "(3693499, 11)\n",
      "(1116443, 9)\n"
     ]
    }
   ],
   "source": [
    "# Queries pour charger les DataFrames\n",
    "query_prod_owners = \"\"\" \n",
    "SELECT\n",
    "    CAST(o.id AS VARCHAR) AS unique_id,\n",
    "    full_name AS owner_fullname,\n",
    "    birth_date AS owner_birth_date,\n",
    "    list_aggregate(o.address_dgfip, 'string_agg', ' ') as owner_address,\n",
    "    kind_class AS owner_category_detail,\n",
    "    CAST(idpersonne AS VARCHAR) AS owner_idpersonne,\n",
    "    city AS owner_city,\n",
    "    postal_code as owner_postal_code,\n",
    "    array_agg(DISTINCT ph.local_id) as local_ids\n",
    "FROM main_stg.stg_production_owners o \n",
    "JOIN production.ban_addresses ba\n",
    "    ON ba.address_kind = 'Owner' AND ba.ref_id = o.id\n",
    "JOIN main_stg.stg_production_owners_housing poh ON poh.owner_id = o.id\n",
    "JOIN main_stg.stg_production_housing ph ON poh.housing_id = ph.id\n",
    "WHERE (occupancy_source = 'V' OR occupancy = 'V') AND data_file_years IS NOT NULL AND list_contains(data_file_years, 'lovac-2024')\n",
    "AND o.birth_date IS NOT NULL\n",
    "GROUP BY o.id, full_name, birth_date, o.address_dgfip, kind_class, idpersonne, city, postal_code\n",
    ";\n",
    "\"\"\"\n",
    "query_ff_owners = \"\"\"\n",
    "WITH idlocal_idprocte AS (\n",
    "    SELECT ff_idlocal, ff_idprocpte\n",
    "    FROM raw_lovac_2024\n",
    "    UNION ALL\n",
    "    SELECT ff_idlocal, ff_idprocpte\n",
    "    FROM raw_lovac_2023\n",
    "),\n",
    "idprocte AS (\n",
    "    SELECT\n",
    "        ff_idprocpte,\n",
    "        array_agg(ff_idlocal) AS ff_idlocals\n",
    "    FROM idlocal_idprocte\n",
    "    GROUP BY ff_idprocpte\n",
    "),\n",
    "owners_local_ids AS (\n",
    "    SELECT\n",
    "        o.idprocpte,\n",
    "        array_agg(ids.ff_idlocals) AS ff_idlocals -- Applatissement des arrays imbriqués\n",
    "    FROM raw_ff_owners o\n",
    "    LEFT JOIN idprocte ids ON o.idprocpte = ids.ff_idprocpte\n",
    "    GROUP BY o.idprocpte\n",
    "),\n",
    "unique_owner_ids AS (\n",
    "    SELECT\n",
    "        o.ff_owner_idpersonne,\n",
    "        unnest(o.ff_owner_idprocpte) AS idprocpte\n",
    "    FROM main_int.int_ff_owners_dedup o\n",
    ")\n",
    "SELECT\n",
    "    CAST(o.ff_owner_idpersonne AS VARCHAR) AS unique_id,\n",
    "    CAST(o.ff_owner_idpersonne AS VARCHAR) AS owner_idpersonne,\n",
    "    ff_owner_address_1 || ' ' || ff_owner_address_2 || ' ' || ff_owner_address_3 || ' ' || ff_owner_address_4 AS owner_address,\n",
    "    ff_owner_postal_code AS owner_postal_code,\n",
    "    ff_owner_birth_date AS owner_birth_date,\n",
    "    ff_owner_lastname AS owner_lastname,\n",
    "    ff_owner_firstname AS owner_firstname,\n",
    "    ff_owner_fullname AS owner_fullname,\n",
    "    ff_owner_category_text AS owner_category_detail,\n",
    "    ff_owner_city AS owner_city,\n",
    "    flatten(flatten(array_agg(DISTINCT li.ff_idlocals))) AS local_ids -- Concaténation des arrays aplatis\n",
    "FROM main_int.int_ff_owners_dedup o\n",
    "JOIN unique_owner_ids u ON u.ff_owner_idpersonne = o.ff_owner_idpersonne\n",
    "JOIN owners_local_ids li ON li.idprocpte = u.idprocpte\n",
    "WHERE ff_owner_birth_date IS NOT NULL\n",
    "GROUP BY\n",
    "    o.ff_owner_idpersonne,\n",
    "    ff_owner_address_1,\n",
    "    ff_owner_address_2,\n",
    "    ff_owner_address_3,\n",
    "    ff_owner_address_4,\n",
    "    ff_owner_postal_code,\n",
    "    ff_owner_birth_date,\n",
    "    ff_owner_lastname,\n",
    "    ff_owner_firstname,\n",
    "    ff_owner_fullname,\n",
    "    ff_owner_category_text,\n",
    "    ff_owner_city;\n",
    "\"\"\"\n",
    "\n",
    "df_ff_owners = connection_source.execute(query_ff_owners).fetchdf()\n",
    "df_prod_owners = connection_source.execute(query_prod_owners).fetchdf()\n",
    "print(df_ff_owners.shape)\n",
    "print(df_prod_owners.shape)\n",
    "\n",
    "connection_source.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import splink.comparison_library as cl\n",
    "from splink import DuckDBAPI, Linker, SettingsCreator, block_on\n",
    "from splink.comparison_level_library import ElseLevel, NullLevel\n",
    "import duckdb\n",
    "\n",
    "# Create DuckDBAPI connection\n",
    "db_api = DuckDBAPI()\n",
    "\n",
    "def local_ids_overlap_comparison():\n",
    "    return {\n",
    "        \"output_column_name\": \"local_ids\",\n",
    "        \"comparison_levels\": [\n",
    "            {\n",
    "                \"sql_condition\": \"local_ids_l IS NULL OR local_ids_r IS NULL\",\n",
    "                \"label_for_charts\": \"Null\",\n",
    "                \"is_null_level\": True,\n",
    "            },\n",
    "            {\n",
    "                \"sql_condition\": \"ARRAY_INTERSECT(local_ids_l, local_ids_r) IS NOT NULL\",\n",
    "                \"label_for_charts\": \"Overlap exists\",\n",
    "                \"tf_adjustment_column\": \"local_ids\",\n",
    "                \"tf_adjustment_weight\": 1.0,\n",
    "            },\n",
    "            {\n",
    "                \"sql_condition\": \"ELSE\",\n",
    "                \"label_for_charts\": \"No overlap\",\n",
    "            },\n",
    "        ],\n",
    "    }\n",
    "\n",
    "\n",
    "# Splink settings\n",
    "settings = SettingsCreator(\n",
    "    link_type=\"link_only\",  # Comparing two datasets\n",
    "    comparisons=[\n",
    "        cl.NameComparison(\"owner_fullname\"),\n",
    "        cl.DateOfBirthComparison(\n",
    "            \"owner_birth_date\",\n",
    "            input_is_string=False,\n",
    "            datetime_metrics=[\"year\", \"month\"],\n",
    "            datetime_thresholds=[1, 2],\n",
    "        ),\n",
    "        cl.LevenshteinAtThresholds(\"owner_address\"),\n",
    "        cl.PostcodeComparison(\"owner_postal_code\"),\n",
    "        cl.ExactMatch(\"owner_city\"),\n",
    "        cl.ExactMatch(\"owner_category_detail\"),\n",
    "        # local_ids_overlap_comparison(),\n",
    "    ],\n",
    "    blocking_rules_to_generate_predictions=[\n",
    "        block_on(\"owner_postal_code\"),\n",
    "        block_on(\"owner_category_detail\"),\n",
    "    ],\n",
    "    retain_intermediate_calculation_columns=False,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Probability two random records match is estimated to be  1.61e-06.\n",
      "This means that amongst all possible pairwise record comparisons, one in 619,980.29 are expected to match.  With 4,123,581,104,057 total possible comparisons, we expect a total of around 6,651,148.75 matching pairs\n"
     ]
    }
   ],
   "source": [
    "# Linker instantiation\n",
    "linker = Linker(\n",
    "    input_table_or_tables=[df_prod_owners, df_ff_owners],\n",
    "    settings=settings,\n",
    "    db_api=db_api,\n",
    ")\n",
    "\n",
    "# Estimate the probability of a match between two random records\n",
    "linker.training.estimate_probability_two_random_records_match(\n",
    "    deterministic_matching_rules=[\n",
    "        block_on(\"owner_postal_code\", \"owner_birth_date\"),\n",
    "        block_on(\"owner_birth_date\"),\n",
    "    ],\n",
    "    recall=0.8,\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "import duckdb\n",
    "duckdb.sql(\"PRAGMA max_temp_directory_size='150GiB'\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "application/vnd.jupyter.widget-view+json": {
       "model_id": "4a08229fb60e44fb9286fe64c9bd83d2",
       "version_major": 2,
       "version_minor": 0
      },
      "text/plain": [
       "FloatProgress(value=0.0, layout=Layout(width='auto'), style=ProgressStyle(bar_color='black'))"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "ename": "SplinkException",
     "evalue": "Error executing the following sql for table `__splink__blocked_id_pairs`(__splink__blocked_id_pairs_dec1593a4):\nCREATE TABLE __splink__blocked_id_pairs_dec1593a4 AS\nWITH __splink__df_concat_with_tf AS (\n  SELECT\n    *\n  FROM __splink__df_concat_with_tf_3acdcc836\n), __splink__df_concat_with_tf_left AS (\n  SELECT\n    *\n  FROM __splink__df_concat_with_tf\n  WHERE\n    source_dataset = (\n      SELECT\n        MIN(source_dataset)\n      FROM __splink__df_concat_with_tf\n    )\n), __splink__df_concat_with_tf_right AS (\n  SELECT\n    *\n  FROM __splink__df_concat_with_tf\n  WHERE\n    source_dataset = (\n      SELECT\n        MAX(source_dataset)\n      FROM __splink__df_concat_with_tf\n    )\n)\nSELECT\n  '0' AS match_key,\n  l.\"source_dataset\" || '-__-' || l.\"unique_id\" AS join_key_l,\n  r.\"source_dataset\" || '-__-' || r.\"unique_id\" AS join_key_r\nFROM __splink__df_concat_with_tf_left AS l\nINNER JOIN __splink__df_concat_with_tf_right AS r\n  ON (\n    l.\"owner_postal_code\" = r.\"owner_postal_code\"\n  )\nWHERE\n  1 = 1\nUNION ALL\nSELECT\n  '1' AS match_key,\n  l.\"source_dataset\" || '-__-' || l.\"unique_id\" AS join_key_l,\n  r.\"source_dataset\" || '-__-' || r.\"unique_id\" AS join_key_r\nFROM __splink__df_concat_with_tf_left AS l\nINNER JOIN __splink__df_concat_with_tf_right AS r\n  ON (\n    l.\"owner_category_detail\" = r.\"owner_category_detail\"\n  )\nWHERE\n  1 = 1\n  AND NOT (\n    COALESCE((\n      l.\"owner_postal_code\" = r.\"owner_postal_code\"\n    ), FALSE)\n  )\n\nError was: Out of Memory Error: failed to offload data block of size 256.0 KiB (245.0 GiB/245.0 GiB used).\nThis limit was set by the 'max_temp_directory_size' setting.\nBy default, this setting utilizes the available disk space on the drive where the 'temp_directory' is located.\nYou can adjust this setting, by using (for example) PRAGMA max_temp_directory_size='10GiB'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mOutOfMemoryException\u001b[0m                      Traceback (most recent call last)",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/splink/internals/database_api.py:63\u001b[0m, in \u001b[0;36mDatabaseAPI._log_and_run_sql_execution\u001b[0;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[1;32m     62\u001b[0m \u001b[38;5;28;01mtry\u001b[39;00m:\n\u001b[0;32m---> 63\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_execute_sql_against_backend\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_sql\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     64\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m \u001b[38;5;28;01mas\u001b[39;00m e:\n\u001b[1;32m     65\u001b[0m     \u001b[38;5;66;03m# Parse our SQL through sqlglot to pretty print\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/splink/internals/duckdb/database_api.py:104\u001b[0m, in \u001b[0;36mDuckDBAPI._execute_sql_against_backend\u001b[0;34m(self, final_sql)\u001b[0m\n\u001b[1;32m    103\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m \u001b[38;5;21m_execute_sql_against_backend\u001b[39m(\u001b[38;5;28mself\u001b[39m, final_sql: \u001b[38;5;28mstr\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m duckdb\u001b[38;5;241m.\u001b[39mDuckDBPyRelation:\n\u001b[0;32m--> 104\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_con\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql\u001b[49m\u001b[43m(\u001b[49m\u001b[43mfinal_sql\u001b[49m\u001b[43m)\u001b[49m\n",
      "\u001b[0;31mOutOfMemoryException\u001b[0m: Out of Memory Error: failed to offload data block of size 256.0 KiB (245.0 GiB/245.0 GiB used).\nThis limit was set by the 'max_temp_directory_size' setting.\nBy default, this setting utilizes the available disk space on the drive where the 'temp_directory' is located.\nYou can adjust this setting, by using (for example) PRAGMA max_temp_directory_size='10GiB'",
      "\nThe above exception was the direct cause of the following exception:\n",
      "\u001b[0;31mSplinkException\u001b[0m                           Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[15], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m pairwise_predictions \u001b[38;5;241m=\u001b[39m \u001b[43mlinker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43minference\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mpredict\u001b[49m\u001b[43m(\u001b[49m\u001b[43mthreshold_match_weight\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[38;5;241;43m-\u001b[39;49m\u001b[38;5;241;43m5\u001b[39;49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/splink/internals/linker_components/inference.py:254\u001b[0m, in \u001b[0;36mLinkerInference.predict\u001b[0;34m(self, threshold_match_probability, threshold_match_weight, materialise_after_computing_term_frequencies, materialise_blocked_pairs)\u001b[0m\n\u001b[1;32m    251\u001b[0m pipeline\u001b[38;5;241m.\u001b[39menqueue_list_of_sqls(sqls)\n\u001b[1;32m    253\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m materialise_blocked_pairs:\n\u001b[0;32m--> 254\u001b[0m     blocked_pairs \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_linker\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_db_api\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql_pipeline_to_splink_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    255\u001b[0m \u001b[43m        \u001b[49m\u001b[43mpipeline\u001b[49m\n\u001b[1;32m    256\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    258\u001b[0m     pipeline \u001b[38;5;241m=\u001b[39m CTEPipeline([blocked_pairs, df_concat_with_tf])\n\u001b[1;32m    259\u001b[0m     blocking_time \u001b[38;5;241m=\u001b[39m time\u001b[38;5;241m.\u001b[39mtime() \u001b[38;5;241m-\u001b[39m start_time\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/splink/internals/database_api.py:200\u001b[0m, in \u001b[0;36mDatabaseAPI.sql_pipeline_to_splink_dataframe\u001b[0;34m(self, pipeline, use_cache)\u001b[0m\n\u001b[1;32m    197\u001b[0m     sql_gen \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39mgenerate_cte_pipeline_sql()\n\u001b[1;32m    198\u001b[0m     output_tablename_templated \u001b[38;5;241m=\u001b[39m pipeline\u001b[38;5;241m.\u001b[39moutput_table_name\n\u001b[0;32m--> 200\u001b[0m     splink_dataframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43msql_to_splink_dataframe_checking_cache\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    201\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql_gen\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    202\u001b[0m \u001b[43m        \u001b[49m\u001b[43moutput_tablename_templated\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    203\u001b[0m \u001b[43m        \u001b[49m\u001b[43muse_cache\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    204\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    205\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[1;32m    206\u001b[0m     \u001b[38;5;66;03m# In debug mode, we do not pipeline the sql and print the\u001b[39;00m\n\u001b[1;32m    207\u001b[0m     \u001b[38;5;66;03m# results of each part of the pipeline\u001b[39;00m\n\u001b[1;32m    208\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m cte \u001b[38;5;129;01min\u001b[39;00m pipeline\u001b[38;5;241m.\u001b[39mctes_pipeline():\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/splink/internals/database_api.py:171\u001b[0m, in \u001b[0;36mDatabaseAPI.sql_to_splink_dataframe_checking_cache\u001b[0;34m(self, sql, output_tablename_templated, use_cache)\u001b[0m\n\u001b[1;32m    168\u001b[0m         \u001b[38;5;28mprint\u001b[39m(df_pd)  \u001b[38;5;66;03m# noqa: T201\u001b[39;00m\n\u001b[1;32m    170\u001b[0m \u001b[38;5;28;01melse\u001b[39;00m:\n\u001b[0;32m--> 171\u001b[0m     splink_dataframe \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_sql_to_splink_dataframe\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    172\u001b[0m \u001b[43m        \u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43moutput_tablename_templated\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtable_name_hash\u001b[49m\n\u001b[1;32m    173\u001b[0m \u001b[43m    \u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    175\u001b[0m splink_dataframe\u001b[38;5;241m.\u001b[39mcreated_by_splink \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mTrue\u001b[39;00m\n\u001b[1;32m    176\u001b[0m splink_dataframe\u001b[38;5;241m.\u001b[39msql_used_to_create \u001b[38;5;241m=\u001b[39m sql\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/splink/internals/database_api.py:93\u001b[0m, in \u001b[0;36mDatabaseAPI._sql_to_splink_dataframe\u001b[0;34m(self, sql, templated_name, physical_name)\u001b[0m\n\u001b[1;32m     85\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     86\u001b[0m \u001b[38;5;124;03mCreate a table in the backend using some given sql\u001b[39;00m\n\u001b[1;32m     87\u001b[0m \n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m     90\u001b[0m \u001b[38;5;124;03mReturns a SplinkDataFrame which also uses templated_name\u001b[39;00m\n\u001b[1;32m     91\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m     92\u001b[0m sql \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_setup_for_execute_sql(sql, physical_name)\n\u001b[0;32m---> 93\u001b[0m spark_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_log_and_run_sql_execution\u001b[49m\u001b[43m(\u001b[49m\u001b[43msql\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtemplated_name\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mphysical_name\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m     94\u001b[0m output_df \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_cleanup_for_execute_sql(\n\u001b[1;32m     95\u001b[0m     spark_df, templated_name, physical_name\n\u001b[1;32m     96\u001b[0m )\n\u001b[1;32m     97\u001b[0m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_intermediate_table_cache\u001b[38;5;241m.\u001b[39mexecuted_queries\u001b[38;5;241m.\u001b[39mappend(output_df)\n",
      "File \u001b[0;32m~/anaconda3/envs/py310/lib/python3.10/site-packages/splink/internals/database_api.py:75\u001b[0m, in \u001b[0;36mDatabaseAPI._log_and_run_sql_execution\u001b[0;34m(self, final_sql, templated_name, physical_name)\u001b[0m\n\u001b[1;32m     72\u001b[0m \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mException\u001b[39;00m:\n\u001b[1;32m     73\u001b[0m     \u001b[38;5;28;01mpass\u001b[39;00m\n\u001b[0;32m---> 75\u001b[0m \u001b[38;5;28;01mraise\u001b[39;00m SplinkException(\n\u001b[1;32m     76\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mError executing the following sql for table \u001b[39m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     77\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124m`\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mtemplated_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m`(\u001b[39m\u001b[38;5;132;01m{\u001b[39;00mphysical_name\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m):\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;132;01m{\u001b[39;00mfinal_sql\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     78\u001b[0m     \u001b[38;5;124mf\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;130;01m\\n\u001b[39;00m\u001b[38;5;124mError was: \u001b[39m\u001b[38;5;132;01m{\u001b[39;00me\u001b[38;5;132;01m}\u001b[39;00m\u001b[38;5;124m\"\u001b[39m\n\u001b[1;32m     79\u001b[0m ) \u001b[38;5;28;01mfrom\u001b[39;00m \u001b[38;5;21;01me\u001b[39;00m\n",
      "\u001b[0;31mSplinkException\u001b[0m: Error executing the following sql for table `__splink__blocked_id_pairs`(__splink__blocked_id_pairs_dec1593a4):\nCREATE TABLE __splink__blocked_id_pairs_dec1593a4 AS\nWITH __splink__df_concat_with_tf AS (\n  SELECT\n    *\n  FROM __splink__df_concat_with_tf_3acdcc836\n), __splink__df_concat_with_tf_left AS (\n  SELECT\n    *\n  FROM __splink__df_concat_with_tf\n  WHERE\n    source_dataset = (\n      SELECT\n        MIN(source_dataset)\n      FROM __splink__df_concat_with_tf\n    )\n), __splink__df_concat_with_tf_right AS (\n  SELECT\n    *\n  FROM __splink__df_concat_with_tf\n  WHERE\n    source_dataset = (\n      SELECT\n        MAX(source_dataset)\n      FROM __splink__df_concat_with_tf\n    )\n)\nSELECT\n  '0' AS match_key,\n  l.\"source_dataset\" || '-__-' || l.\"unique_id\" AS join_key_l,\n  r.\"source_dataset\" || '-__-' || r.\"unique_id\" AS join_key_r\nFROM __splink__df_concat_with_tf_left AS l\nINNER JOIN __splink__df_concat_with_tf_right AS r\n  ON (\n    l.\"owner_postal_code\" = r.\"owner_postal_code\"\n  )\nWHERE\n  1 = 1\nUNION ALL\nSELECT\n  '1' AS match_key,\n  l.\"source_dataset\" || '-__-' || l.\"unique_id\" AS join_key_l,\n  r.\"source_dataset\" || '-__-' || r.\"unique_id\" AS join_key_r\nFROM __splink__df_concat_with_tf_left AS l\nINNER JOIN __splink__df_concat_with_tf_right AS r\n  ON (\n    l.\"owner_category_detail\" = r.\"owner_category_detail\"\n  )\nWHERE\n  1 = 1\n  AND NOT (\n    COALESCE((\n      l.\"owner_postal_code\" = r.\"owner_postal_code\"\n    ), FALSE)\n  )\n\nError was: Out of Memory Error: failed to offload data block of size 256.0 KiB (245.0 GiB/245.0 GiB used).\nThis limit was set by the 'max_temp_directory_size' setting.\nBy default, this setting utilizes the available disk space on the drive where the 'temp_directory' is located.\nYou can adjust this setting, by using (for example) PRAGMA max_temp_directory_size='10GiB'"
     ]
    }
   ],
   "source": [
    "pairwise_predictions = linker.inference.predict(threshold_match_weight=-5)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'pairwise_predictions' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[4], line 1\u001b[0m\n\u001b[0;32m----> 1\u001b[0m \u001b[43mpairwise_predictions\u001b[49m\n",
      "\u001b[0;31mNameError\u001b[0m: name 'pairwise_predictions' is not defined"
     ]
    }
   ],
   "source": [
    "pairwise_predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.15"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
