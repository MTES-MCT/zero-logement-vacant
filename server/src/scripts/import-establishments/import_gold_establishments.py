#!/usr/bin/env python3
"""
Import establishments from Gold layer CSV into the establishments table.

This script imports collectivities data from the gold_establishments CSV file
generated by the analytics pipeline (collectivities.py + entities.py).

IMPLEMENTATION RULES:
- R1: Join key = SIREN (unique constraint in DB)
- R2: Skip rows with missing/empty SIREN
- R3: Geo_Perimeter → localities_geo_code (Python list string → PostgreSQL array)
- R4: Name-zlv → name
- R5: Kind-admin → kind (direct mapping, no transformation)
- R6: Source = "gold_establishments_{Millesime}"
- R8: Idempotent: UPDATE if SIREN exists, INSERT otherwise
- R9: Dry-run mode required for validation
- R10: Log all skipped rows and errors

Usage:
    # Dry-run mode (validation only)
    python import_gold_establishments.py --csv collectivities_processed.csv --db-url postgresql://user:pass@host:port/db --dry-run

    # Execute import with limit
    python import_gold_establishments.py --csv collectivities_processed.csv --db-url postgresql://user:pass@host:port/db --limit 100

    # Full import
    python import_gold_establishments.py --csv collectivities_processed.csv --db-url postgresql://user:pass@host:port/db
"""

import argparse
import ast
import csv
import logging
import sys
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import psycopg2
from psycopg2.extras import RealDictCursor, execute_values
from tqdm import tqdm


class EstablishmentImporter:
    """Import establishments from Gold layer CSV."""

    def __init__(
        self,
        db_url: str,
        csv_path: str,
        dry_run: bool = False,
        batch_size: int = 1000,
    ):
        """
        Initialize the importer.

        Args:
            db_url: PostgreSQL connection URI
            csv_path: Path to the gold establishments CSV
            dry_run: If True, validate without writing to DB
            batch_size: Number of records per batch
        """
        self.db_url = db_url
        self.csv_path = csv_path
        self.dry_run = dry_run
        self.batch_size = batch_size
        self.conn = None
        self.cursor = None

        # Statistics
        self.stats = {
            "total": 0,
            "inserted": 0,
            "updated": 0,
            "skipped_no_siren": 0,
            "skipped_invalid_siren": 0,
            "errors": 0,
        }

        # Track skipped rows for reporting
        self.skipped_rows: List[Dict] = []

    def connect(self):
        """Establish database connection."""
        try:
            self.conn = psycopg2.connect(self.db_url)
            self.cursor = self.conn.cursor(cursor_factory=RealDictCursor)
            logging.info("Database connection established")
        except Exception as e:
            logging.error(f"Database connection failed: {e}")
            raise

    def disconnect(self):
        """Close database connection."""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()

    def parse_geo_perimeter(self, value: str) -> Optional[List[str]]:
        """
        Parse Geo_Perimeter from Python list string to actual list.

        Args:
            value: String like "['97101', '97102', ...]"

        Returns:
            List of commune codes or None if parsing fails
        """
        if not value or value == "":
            return None

        try:
            # Parse Python list literal
            parsed = ast.literal_eval(value)
            if isinstance(parsed, list):
                return [str(code) for code in parsed]
            return None
        except (ValueError, SyntaxError):
            logging.debug(f"Failed to parse Geo_Perimeter: {value[:50]}...")
            return None

    def parse_siren(self, row: Dict) -> Optional[int]:
        """
        Extract SIREN from row (use Siren column, or truncate Siret).

        Args:
            row: CSV row dict

        Returns:
            SIREN as integer, or None if invalid
        """
        siren = row.get("Siren", "")

        # R2: Skip if SIREN is missing
        if not siren or siren.strip() == "":
            return None

        siren_str = str(siren).strip()

        # Handle potential decimal format (e.g., "239710015.0")
        if "." in siren_str:
            siren_str = siren_str.split(".")[0]

        # Validate: SIREN should be 9 digits
        if not siren_str.isdigit() or len(siren_str) != 9:
            return None

        try:
            return int(siren_str)
        except ValueError:
            return None

    def transform_row(self, row: Dict) -> Optional[Dict]:
        """
        Transform CSV row to DB record.

        Args:
            row: CSV row dict

        Returns:
            Transformed record or None if should be skipped
        """
        self.stats["total"] += 1

        # R1 & R2: Parse SIREN
        siren = self.parse_siren(row)
        if siren is None:
            siren_value = row.get("Siren", "<empty>")
            self.stats["skipped_no_siren"] += 1
            self.skipped_rows.append(
                {
                    "reason": "missing_or_invalid_siren",
                    "siren": siren_value,
                    "name": row.get("Name-zlv", ""),
                    "kind": row.get("Kind-admin", ""),
                }
            )
            return None

        # R3: Parse Geo_Perimeter
        localities = self.parse_geo_perimeter(row.get("Geo_Perimeter", ""))

        # R4: Name
        name = row.get("Name-zlv", "").strip()
        if not name:
            name = row.get("Name-source", "Unknown")

        # R5: Kind (direct mapping)
        kind = row.get("Kind-admin", "").strip()
        if not kind:
            kind = "UNKNOWN"

        # R6: Source with millesime
        millesime = row.get("Millesime", "2025")
        source = f"gold_establishments_{millesime}"

        return {
            "siren": siren,
            "name": name[:255],  # Truncate to VARCHAR(255)
            "kind": kind[:255],
            "localities_geo_code": localities,
            "available": True,
            "source": source,
        }

    def load_csv(self, limit: Optional[int] = None) -> List[Dict]:
        """
        Load and transform CSV data.

        Args:
            limit: Maximum number of rows to process

        Returns:
            List of transformed records
        """
        records = []
        row_count = 0

        with open(self.csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)

            for row in tqdm(reader, desc="Loading CSV", unit="rows"):
                if limit and row_count >= limit:
                    break

                row_count += 1
                record = self.transform_row(row)
                if record:
                    records.append(record)

        return records

    def get_existing_sirens(self, sirens: List[int]) -> set:
        """
        Get SIRENs that already exist in the database.

        Args:
            sirens: List of SIREN values to check

        Returns:
            Set of existing SIRENs
        """
        if not sirens:
            return set()

        self.cursor.execute(
            """
            SELECT siren FROM establishments
            WHERE siren = ANY(%s) AND siren IS NOT NULL
            """,
            (sirens,),
        )

        return {row["siren"] for row in self.cursor.fetchall()}

    def batch_upsert(self, records: List[Dict]):
        """
        Perform batch upsert: INSERT new records, UPDATE existing ones.

        Args:
            records: List of records to upsert
        """
        if not records:
            return

        # Separate inserts and updates
        sirens = [r["siren"] for r in records]
        existing_sirens = self.get_existing_sirens(sirens)

        to_insert = [r for r in records if r["siren"] not in existing_sirens]
        to_update = [r for r in records if r["siren"] in existing_sirens]

        # INSERT new records
        if to_insert and not self.dry_run:
            insert_data = [
                (
                    r["siren"],
                    r["name"],
                    r["kind"],
                    r["localities_geo_code"],
                    r["available"],
                    r["source"],
                )
                for r in to_insert
            ]

            execute_values(
                self.cursor,
                """
                INSERT INTO establishments
                    (siren, name, kind, localities_geo_code, available, source, updated_at)
                VALUES %s
                """,
                insert_data,
                template="(%s, %s, %s, %s, %s, %s, NOW())",
                page_size=500,
            )

        self.stats["inserted"] += len(to_insert)

        # UPDATE existing records
        if to_update and not self.dry_run:
            update_data = [
                (
                    r["name"],
                    r["kind"],
                    r["localities_geo_code"],
                    r["source"],
                    r["siren"],
                )
                for r in to_update
            ]

            execute_values(
                self.cursor,
                """
                UPDATE establishments AS e
                SET
                    name = data.name,
                    kind = data.kind,
                    localities_geo_code = data.localities_geo_code,
                    source = data.source,
                    updated_at = NOW()
                FROM (VALUES %s) AS data(name, kind, localities_geo_code, source, siren)
                WHERE e.siren = data.siren::integer
                """,
                update_data,
                page_size=500,
            )

        self.stats["updated"] += len(to_update)

    def run(self, limit: Optional[int] = None):
        """
        Main execution method.

        Args:
            limit: Maximum number of records to process
        """
        print("=" * 80)
        print("GOLD ESTABLISHMENTS IMPORTER")
        print("=" * 80)
        print(f"CSV file: {self.csv_path}")
        print(f"Dry-run: {self.dry_run}")
        print(f"Limit: {limit if limit else 'None'}")
        print()

        self.connect()

        try:
            # Load and transform CSV
            records = self.load_csv(limit)
            print(f"\n{len(records):,} valid records loaded")

            if not records:
                print("No records to process")
                return

            # Process in batches
            print(f"\nProcessing in batches of {self.batch_size}...")

            for i in tqdm(
                range(0, len(records), self.batch_size),
                desc="Batches",
                unit="batch",
            ):
                batch = records[i : i + self.batch_size]
                self.batch_upsert(batch)

                # Commit per batch
                if not self.dry_run:
                    self.conn.commit()

            # Print summary
            print("\n" + "=" * 80)
            print("SUMMARY")
            print("=" * 80)
            print(f"Total rows processed: {self.stats['total']:,}")
            print(f"Inserted: {self.stats['inserted']:,}")
            print(f"Updated: {self.stats['updated']:,}")
            print(f"Skipped (no SIREN): {self.stats['skipped_no_siren']:,}")
            print(f"Errors: {self.stats['errors']:,}")

            if self.dry_run:
                print("\n[DRY-RUN] No changes were made to the database")

            # Report skipped rows
            if self.skipped_rows:
                print(f"\n--- Skipped rows sample (first 10) ---")
                for row in self.skipped_rows[:10]:
                    print(
                        f"  {row['reason']}: SIREN={row['siren']}, name={row['name'][:40]}"
                    )

                if len(self.skipped_rows) > 10:
                    print(f"  ... and {len(self.skipped_rows) - 10} more")

        except Exception as e:
            logging.error(f"Import failed: {e}")
            if self.conn:
                self.conn.rollback()
            raise

        finally:
            self.disconnect()


def main():
    parser = argparse.ArgumentParser(
        description="Import establishments from Gold layer CSV"
    )

    # Required arguments
    parser.add_argument(
        "--csv",
        required=True,
        help="Path to the gold establishments CSV file",
    )
    parser.add_argument(
        "--db-url",
        required=True,
        help="PostgreSQL connection URI (postgresql://user:pass@host:port/dbname)",
    )

    # Optional arguments
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Validate without writing to database",
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="Limit number of rows to process",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="Batch size for database operations (default: 1000)",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging",
    )

    args = parser.parse_args()

    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.WARNING,
        format="%(levelname)s - %(message)s",
    )

    # Create and run importer
    importer = EstablishmentImporter(
        db_url=args.db_url,
        csv_path=args.csv,
        dry_run=args.dry_run,
        batch_size=args.batch_size,
    )

    try:
        importer.run(limit=args.limit)
        print("\n" + ("=" * 80))
        if args.dry_run:
            print("DRY-RUN completed successfully")
        else:
            print("Import completed successfully")
        print("=" * 80)
    except KeyboardInterrupt:
        print("\nInterrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nFailed: {e}")
        if args.debug:
            import traceback

            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
