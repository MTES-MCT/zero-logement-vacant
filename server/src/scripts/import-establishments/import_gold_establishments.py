#!/usr/bin/env python3
"""
Import establishments from Gold layer CSV into the establishments table.

This script imports collectivities data from the gold_establishments CSV file
generated by the analytics pipeline (collectivities.py + entities.py).

IMPLEMENTATION RULES:
- R1: Join key = SIREN (unique constraint in DB)
- R2: Skip rows with missing/empty SIREN
- R3: Geo_Perimeter → localities_geo_code (Python list string → PostgreSQL array)
- R4: Name-zlv → name
- R5: Kind-admin → kind (direct mapping, no transformation)
- R6: Source = "gold_establishments_{Millesime}"
- R8: Idempotent: UPDATE if SIREN exists, INSERT otherwise
- R9: Dry-run mode required for validation
- R10: Log all skipped rows and errors

Usage:
    # Dry-run mode (validation only)
    python import_gold_establishments.py --csv collectivities_processed.csv --db-url postgresql://user:pass@host:port/db --dry-run

    # Execute import with limit
    python import_gold_establishments.py --csv collectivities_processed.csv --db-url postgresql://user:pass@host:port/db --limit 100

    # Full import
    python import_gold_establishments.py --csv collectivities_processed.csv --db-url postgresql://user:pass@host:port/db
"""

import argparse
import ast
import csv
import logging
import sys
from datetime import datetime
from typing import Dict, List, Optional, Tuple

import psycopg2
from psycopg2.extras import RealDictCursor, execute_values
from tqdm import tqdm


class EstablishmentImporter:
    """Import establishments from Gold layer CSV."""

    def __init__(
        self,
        db_url: str,
        csv_path: str,
        dry_run: bool = False,
        batch_size: int = 1000,
    ):
        """
        Initialize the importer.

        Args:
            db_url: PostgreSQL connection URI
            csv_path: Path to the gold establishments CSV
            dry_run: If True, validate without writing to DB
            batch_size: Number of records per batch
        """
        self.db_url = db_url
        self.csv_path = csv_path
        self.dry_run = dry_run
        self.batch_size = batch_size
        self.conn = None
        self.cursor = None

        # Statistics
        self.stats = {
            "total": 0,
            "inserted": 0,
            "updated": 0,
            "skipped_no_siren": 0,
            "skipped_invalid_siren": 0,
            "skipped_invalid_localities": 0,
            "errors": 0,
        }

        # Track skipped rows for reporting
        self.skipped_rows: List[Dict] = []

        # Valid locality geo_codes from DB (loaded at connect)
        self.valid_localities: set = set()

    def connect(self):
        """Establish database connection."""
        try:
            self.conn = psycopg2.connect(self.db_url)
            self.cursor = self.conn.cursor(cursor_factory=RealDictCursor)
            logging.info("Database connection established")
        except Exception as e:
            logging.error(f"Database connection failed: {e}")
            raise

    def load_valid_localities(self):
        """Load all valid geo_codes from the localities table."""
        self.cursor.execute("SELECT geo_code FROM localities")
        self.valid_localities = {row["geo_code"] for row in self.cursor.fetchall()}
        print(f"Loaded {len(self.valid_localities):,} valid localities from database")

    def validate_localities(self, geo_codes: Optional[List[str]]) -> Tuple[bool, List[str]]:
        """
        Validate that all geo_codes exist in the localities table.

        Args:
            geo_codes: List of geo codes to validate

        Returns:
            Tuple of (is_valid, list_of_invalid_codes)
        """
        if not geo_codes:
            return True, []

        invalid_codes = [code for code in geo_codes if code not in self.valid_localities]
        return len(invalid_codes) == 0, invalid_codes

    def disconnect(self):
        """Close database connection."""
        if self.cursor:
            self.cursor.close()
        if self.conn:
            self.conn.close()

    def parse_geo_perimeter(self, value: str) -> Optional[List[str]]:
        """
        Parse Geo_Perimeter from Python list string to actual list.

        Args:
            value: String like "['97101', '97102', ...]"

        Returns:
            List of commune codes or None if parsing fails
        """
        if not value or value == "":
            return None

        try:
            # Parse Python list literal
            parsed = ast.literal_eval(value)
            if isinstance(parsed, list):
                return [str(code) for code in parsed]
            return None
        except (ValueError, SyntaxError):
            logging.debug(f"Failed to parse Geo_Perimeter: {value[:50]}...")
            return None

    def parse_siren(self, row: Dict) -> Optional[int]:
        """
        Extract SIREN from row (use Siren column, or truncate Siret).

        Args:
            row: CSV row dict

        Returns:
            SIREN as integer, or None if invalid
        """
        siren = row.get("Siren", "")

        # R2: Skip if SIREN is missing
        if not siren or siren.strip() == "":
            return None

        siren_str = str(siren).strip()

        # Handle potential decimal format (e.g., "239710015.0")
        if "." in siren_str:
            siren_str = siren_str.split(".")[0]

        # Validate: SIREN should be 9 digits
        if not siren_str.isdigit() or len(siren_str) != 9:
            return None

        try:
            return int(siren_str)
        except ValueError:
            return None

    def parse_siret(self, row: Dict) -> Optional[str]:
        """
        Extract and validate SIRET from row.

        Args:
            row: CSV row dict

        Returns:
            SIRET as string (14 digits), or None if invalid
        """
        siret = row.get("Siret", "")

        if not siret or str(siret).strip() == "":
            return None

        siret_str = str(siret).strip()

        # Handle potential decimal format (e.g., "23971001500015.0")
        if "." in siret_str:
            siret_str = siret_str.split(".")[0]

        # Validate: SIRET should be 14 digits
        if not siret_str.isdigit() or len(siret_str) != 14:
            return None

        return siret_str

    def compute_short_name(self, name: str, kind: str) -> str:
        """
        Compute short name from full name.

        For communes, removes "Commune de " or "Commune d'" prefix.

        Args:
            name: Full establishment name
            kind: Establishment kind (COM, COM-TOM, etc.)

        Returns:
            Short name
        """
        import re

        if kind in ("COM", "COM-TOM"):
            return re.sub(r"^Commune d(e\s|')", "", name)
        return name

    def normalize_name(self, name: str) -> str:
        """
        Normalize establishment name from legacy uppercase to title case.

        Handles French naming conventions:
        - Lowercase articles: de, du, des, d', l', la, le, les, et, en, sur, sous
        - Preserves acronyms in parentheses: (DDETS), (DDETSPP), (DDT)
        - Preserves department names after hyphens

        Examples:
            "PREFECTURE DE DEPARTEMENT COTE-D'OR" -> "Préfecture de Département Côte-d'Or"
            "COMMUNE DE PARIS" -> "Commune de Paris"

        Args:
            name: Original name (possibly all uppercase)

        Returns:
            Normalized name with proper capitalization
        """
        import re

        # If not all uppercase, return as-is (already normalized)
        if name != name.upper():
            return name

        # French lowercase words (articles, prepositions)
        lowercase_words = {
            "de", "du", "des", "d", "l", "la", "le", "les",
            "et", "en", "sur", "sous", "aux", "au", "a"
        }

        # Extract and preserve acronyms in parentheses
        acronyms = {}
        def save_acronym(match):
            key = f"__ACRONYM_{len(acronyms)}__"
            acronyms[key] = match.group(0)
            return key

        # Save acronyms like (DDETS), (DDT), etc.
        name = re.sub(r'\([A-Z]{2,}\)', save_acronym, name)

        # Convert to title case
        words = name.lower().split()
        result = []

        for i, word in enumerate(words):
            # Check if it's a placeholder for an acronym
            if word.startswith("__acronym_"):
                result.append(word.upper())
                continue

            # Handle hyphenated words (e.g., "côte-d'or")
            if "-" in word:
                parts = word.split("-")
                normalized_parts = []
                for j, part in enumerate(parts):
                    # Handle d' and l' contractions
                    if part in ("d", "l") and j < len(parts) - 1:
                        normalized_parts.append(part)
                    elif j == 0 or part not in lowercase_words:
                        normalized_parts.append(part.capitalize())
                    else:
                        normalized_parts.append(part)
                result.append("-".join(normalized_parts))
            # First word is always capitalized
            elif i == 0:
                result.append(word.capitalize())
            # Lowercase articles/prepositions in middle of name
            elif word in lowercase_words:
                result.append(word)
            else:
                result.append(word.capitalize())

        normalized = " ".join(result)

        # Restore acronyms
        for key, value in acronyms.items():
            normalized = normalized.replace(key.upper(), value)
            normalized = normalized.replace(key.lower(), value)
            normalized = normalized.replace(key.capitalize(), value)

        return normalized

    def transform_row(self, row: Dict) -> Optional[Dict]:
        """
        Transform CSV row to DB record.

        Args:
            row: CSV row dict

        Returns:
            Transformed record or None if should be skipped
        """
        self.stats["total"] += 1

        # R1 & R2: Parse SIREN
        siren = self.parse_siren(row)
        if siren is None:
            siren_value = row.get("Siren", "<empty>")
            self.stats["skipped_no_siren"] += 1
            self.skipped_rows.append(
                {
                    "reason": "missing_or_invalid_siren",
                    "siren": siren_value,
                    "name": row.get("Name-zlv", ""),
                    "kind": row.get("Kind-admin", ""),
                }
            )
            return None

        # R3: Parse Geo_Perimeter
        localities = self.parse_geo_perimeter(row.get("Geo_Perimeter", ""))

        # Validate localities against DB
        is_valid, invalid_codes = self.validate_localities(localities)
        if not is_valid:
            self.stats["skipped_invalid_localities"] += 1
            self.skipped_rows.append(
                {
                    "reason": "invalid_localities",
                    "siren": str(siren),
                    "name": row.get("Name-zlv", ""),
                    "kind": row.get("Kind-admin", ""),
                    "invalid_codes": invalid_codes[:5],  # Show first 5 invalid codes
                    "total_invalid": len(invalid_codes),
                }
            )
            logging.error(
                f"SIREN {siren}: {len(invalid_codes)} invalid locality codes - "
                f"first 5: {invalid_codes[:5]}"
            )
            return None

        # R4: Name (normalize from uppercase legacy format)
        name = row.get("Name-zlv", "").strip()
        if not name:
            name = row.get("Name-source", "Unknown")
        name = self.normalize_name(name)

        # R5: Kind (direct mapping)
        kind = row.get("Kind-admin", "").strip()
        if not kind:
            kind = "UNKNOWN"

        # R6: Source with millesime
        millesime = row.get("Millesime", "2025").strip() if row.get("Millesime") else "2025"
        source = f"gold_establishments_{millesime}"

        # New fields
        siret = self.parse_siret(row)
        short_name = self.compute_short_name(name, kind)
        kind_admin_meta = row.get("Kind-admin_meta", "").strip() or None

        return {
            "siren": siren,
            "siret": siret,
            "name": name[:255],  # Truncate to VARCHAR(255)
            "short_name": short_name[:255],
            "kind": kind[:255],
            "kind_admin_meta": kind_admin_meta[:50] if kind_admin_meta else None,
            "millesime": millesime[:4] if millesime else None,
            "localities_geo_code": localities,
            "available": True,
            "source": source,
        }

    def load_csv(self, limit: Optional[int] = None) -> List[Dict]:
        """
        Load and transform CSV data.

        Args:
            limit: Maximum number of rows to process

        Returns:
            List of transformed records
        """
        records = []
        row_count = 0

        with open(self.csv_path, "r", encoding="utf-8") as f:
            reader = csv.DictReader(f)

            for row in tqdm(reader, desc="Loading CSV", unit="rows"):
                if limit and row_count >= limit:
                    break

                row_count += 1
                record = self.transform_row(row)
                if record:
                    records.append(record)

        return records

    def get_existing_sirens(self, sirens: List[int]) -> set:
        """
        Get SIRENs that already exist in the database.

        Args:
            sirens: List of SIREN values to check

        Returns:
            Set of existing SIRENs
        """
        if not sirens:
            return set()

        self.cursor.execute(
            """
            SELECT siren FROM establishments
            WHERE siren = ANY(%s) AND siren IS NOT NULL
            """,
            (sirens,),
        )

        return {row["siren"] for row in self.cursor.fetchall()}

    def batch_upsert(self, records: List[Dict]):
        """
        Perform batch upsert: INSERT new records, UPDATE existing ones.

        Args:
            records: List of records to upsert
        """
        if not records:
            return

        # Separate inserts and updates
        sirens = [r["siren"] for r in records]
        existing_sirens = self.get_existing_sirens(sirens)

        to_insert = [r for r in records if r["siren"] not in existing_sirens]
        to_update = [r for r in records if r["siren"] in existing_sirens]

        # INSERT new records
        if to_insert and not self.dry_run:
            insert_data = [
                (
                    r["siren"],
                    r["siret"],
                    r["name"],
                    r["short_name"],
                    r["kind"],
                    r["kind_admin_meta"],
                    r["millesime"],
                    r["localities_geo_code"],
                    r["available"],
                    r["source"],
                )
                for r in to_insert
            ]

            execute_values(
                self.cursor,
                """
                INSERT INTO establishments
                    (siren, siret, name, short_name, kind, kind_admin_meta, millesime,
                     localities_geo_code, available, source, updated_at)
                VALUES %s
                """,
                insert_data,
                template="(%s, %s, %s, %s, %s, %s, %s, %s, %s, %s, NOW())",
                page_size=500,
            )

        self.stats["inserted"] += len(to_insert)

        # UPDATE existing records
        if to_update and not self.dry_run:
            update_data = [
                (
                    r["siret"],
                    r["name"],
                    r["short_name"],
                    r["kind"],
                    r["kind_admin_meta"],
                    r["millesime"],
                    r["localities_geo_code"],
                    r["source"],
                    r["siren"],
                )
                for r in to_update
            ]

            execute_values(
                self.cursor,
                """
                UPDATE establishments AS e
                SET
                    siret = data.siret,
                    name = data.name,
                    short_name = data.short_name,
                    kind = data.kind,
                    kind_admin_meta = data.kind_admin_meta,
                    millesime = data.millesime,
                    localities_geo_code = data.localities_geo_code,
                    source = data.source,
                    updated_at = NOW()
                FROM (VALUES %s) AS data(siret, name, short_name, kind, kind_admin_meta, millesime, localities_geo_code, source, siren)
                WHERE e.siren = data.siren::integer
                """,
                update_data,
                page_size=500,
            )

        self.stats["updated"] += len(to_update)

    def run(self, limit: Optional[int] = None):
        """
        Main execution method.

        Args:
            limit: Maximum number of records to process
        """
        print("=" * 80)
        print("GOLD ESTABLISHMENTS IMPORTER")
        print("=" * 80)
        print(f"CSV file: {self.csv_path}")
        print(f"Dry-run: {self.dry_run}")
        print(f"Limit: {limit if limit else 'None'}")
        print()

        self.connect()

        try:
            # Load valid localities for integrity check
            self.load_valid_localities()

            # Load and transform CSV
            records = self.load_csv(limit)
            print(f"\n{len(records):,} valid records loaded")

            if not records:
                print("No records to process")
                return

            # Process in batches
            print(f"\nProcessing in batches of {self.batch_size}...")

            for i in tqdm(
                range(0, len(records), self.batch_size),
                desc="Batches",
                unit="batch",
            ):
                batch = records[i : i + self.batch_size]
                self.batch_upsert(batch)

                # Commit per batch
                if not self.dry_run:
                    self.conn.commit()

            # Print summary
            print("\n" + "=" * 80)
            print("SUMMARY")
            print("=" * 80)
            print(f"Total rows processed: {self.stats['total']:,}")
            print(f"Inserted: {self.stats['inserted']:,}")
            print(f"Updated: {self.stats['updated']:,}")
            print(f"Skipped (no SIREN): {self.stats['skipped_no_siren']:,}")
            print(f"Skipped (invalid localities): {self.stats['skipped_invalid_localities']:,}")
            print(f"Errors: {self.stats['errors']:,}")

            if self.dry_run:
                print("\n[DRY-RUN] No changes were made to the database")

            # Report skipped rows
            if self.skipped_rows:
                print(f"\n--- Skipped rows sample (first 10) ---")
                for row in self.skipped_rows[:10]:
                    if row["reason"] == "invalid_localities":
                        print(
                            f"  {row['reason']}: SIREN={row['siren']}, name={row['name'][:30]}, "
                            f"{row['total_invalid']} invalid codes: {row['invalid_codes']}"
                        )
                    else:
                        print(
                            f"  {row['reason']}: SIREN={row['siren']}, name={row['name'][:40]}"
                        )

                if len(self.skipped_rows) > 10:
                    print(f"  ... and {len(self.skipped_rows) - 10} more")

        except Exception as e:
            logging.error(f"Import failed: {e}")
            if self.conn:
                self.conn.rollback()
            raise

        finally:
            self.disconnect()


def main():
    parser = argparse.ArgumentParser(
        description="Import establishments from Gold layer CSV"
    )

    # Required arguments
    parser.add_argument(
        "--csv",
        required=True,
        help="Path to the gold establishments CSV file",
    )
    parser.add_argument(
        "--db-url",
        required=True,
        help="PostgreSQL connection URI (postgresql://user:pass@host:port/dbname)",
    )

    # Optional arguments
    parser.add_argument(
        "--dry-run",
        action="store_true",
        help="Validate without writing to database",
    )
    parser.add_argument(
        "--limit",
        type=int,
        help="Limit number of rows to process",
    )
    parser.add_argument(
        "--batch-size",
        type=int,
        default=1000,
        help="Batch size for database operations (default: 1000)",
    )
    parser.add_argument(
        "--debug",
        action="store_true",
        help="Enable debug logging",
    )

    args = parser.parse_args()

    # Configure logging
    logging.basicConfig(
        level=logging.DEBUG if args.debug else logging.WARNING,
        format="%(levelname)s - %(message)s",
    )

    # Create and run importer
    importer = EstablishmentImporter(
        db_url=args.db_url,
        csv_path=args.csv,
        dry_run=args.dry_run,
        batch_size=args.batch_size,
    )

    try:
        importer.run(limit=args.limit)
        print("\n" + ("=" * 80))
        if args.dry_run:
            print("DRY-RUN completed successfully")
        else:
            print("Import completed successfully")
        print("=" * 80)
    except KeyboardInterrupt:
        print("\nInterrupted by user")
        sys.exit(1)
    except Exception as e:
        print(f"\nFailed: {e}")
        if args.debug:
            import traceback

            traceback.print_exc()
        sys.exit(1)


if __name__ == "__main__":
    main()
